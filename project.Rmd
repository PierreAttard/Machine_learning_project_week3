---
title: "project"
output: html_document
---
#Practical Machine Learning
We'll study the manner in which they did the exercise.



```{r echo = FALSE}
library(AppliedPredictiveModeling)
library(caret)
library(rpart)
library(rpart.plot)
library(ggplot2)
library(rattle)
library(Hmisc)
library(foreach)
library(randomForest)
library(doParallel)
```

#Getting and Clearing data
There is missing values. We have to clean data to have data we can work with. 
We delete also the variable which are useless for the calculation of our question. 
```{r}
training<-read.csv("pml-training.csv", header = TRUE, na.strings= c("NA", "", "#DIV/0!"))
training<-training[, colSums(is.na(training)) == 0]
test    <-read.csv("pml-testing.csv",  header = TRUE, na.strings= c("NA", "", "#DIV/0!"))
test    <-test[,     colSums(is.na(test))     == 0]
non_using_variable<- grep("timestamp|X|user_name|new_window", names(training))
training<- training[, -non_using_variable]
test    <- test    [, -non_using_variable] 
```
We do the partition
```{r}
inTrain <- createDataPartition(y = training$classe, p = 0.7, list = FALSE)
training<-training[inTrain,]
testing <-training[-inTrain,]
nrow(testing)
nrow(training)
```
#Random forest

We will run the Random Forest algorithme. 

```{r}
FIT_rf <- train(classe ~ ., method = "rf", data = training)
```
Now we'll see the quality of our algorithm:
```{r}
preds <- predict(FIT_rf, newdata = testing)
confusionMatrix(preds, testing$classe)
```

#Decision Tree
```{r}
FIT_class_tree <- train(classe ~ ., method="rpart", data = training)
fancyRpartPlot(FIT_class_tree$finalModel)
```

#Generating files
We use the random forest algorithm to submot our work to the "Machine"!
```{r}
pred_submission<-as.character(predict(FIT_rf, test))

pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(pred_submission)
```